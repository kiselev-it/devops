# Задание 1 
## Составьте постмотрем, на основе реального сбоя системы Github в 2018 году
1. Краткое описание инцидента (краткая выжимка о инциденте:    
22 октября 2018 г. произошел инцидент , в результате которого обслуживание ухудшилось на 24 часа 11 минут. Было затронуто несколько внутренних систем, в результате чего отображалась устаревшая и непоследовательная информация, в частности, после нескольких попыток создания Issue и записи комментариев к уже существующим — столкнулся с тем что запись/комментарий визуально отображаются а после перезагрузки — пропадают.. В конечном итоге никакие пользовательские данные не были потеряны.  

2. Предшествующие события (что произошло перед инцидентом):  
В 22:52 (21.10) по UTC на нескольких сервисах GitHub пострадали несколько сетевых разделов и последующим сбоем базы данных.

3. Причина инцидента (из-за чего возник инцидент):  
Плановые работы по техническому обслуживанию по замене вышедшего из строя оптического оборудования 100G привели к потере связи между сетевым центром на восточном побережье США и основным центром обработки данных на восточном побережье США.  

4. Воздействие (на что повлиял инцидент):  
Повлиял на работу серверов. В чстаности, на нескольких сервисах GitHub пострадали несколько сетевых разделов и последующим сбоем базы данных.  

5. Обнаружение (когда и как инцидент был обнаружен):  
Внутренние системы мониторинга начали генерировать предупреждения, указывающие на то, что в системах возникло множество неисправностей. 

6. Реакция (кто ответил на инцидент, кто был привлечен, какие каналы коммуникации были задействованы):  
В это время несколько инженеров работали над сортировкой входящих уведомлений. Были привлечены дополнительные инженеры из команды разработчиков, responding team, координатор инцидента.

7. Восстановление (описание действий по устранению инцидента и поведение системы):  
Были предприняты шаги для обеспечения целостности пользотельских данных, включая приостановку работы событий (hooks) и других внутренних систем обработки. Также был запущен процесс востановление баз данных из резервных копий.

8. Таймлайн (последовательное описание ключевых событий инцидента с указанием времени):  
21 октября 2018, 22:52 UTC -  Orchestrator приступил к организации топологий кластеров баз данных Западного побережья США.    
21 октября 2018, 22:54 UTC - Наши внутренние системы мониторинга начали генерировать предупреждения, указывающие на то, что в наших системах возникло множество неисправностей.  
21 октября 2018, 23:07 UTC - К этому моменту команда разработчиков решила вручную заблокировать наши внутренние инструменты развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений.  
21 октября 2018 г., 23:13 UTC - Инженеры начали исследовать текущее состояние, чтобы определить, какие действия необходимо предпринять, чтобы вручную настроить базу данных Восточного побережья США в качестве основной для каждого кластера и перестроить топологию репликации.   
21 Октября 2018 23:19 UTC - Посредством запроса состояния кластеров базы данных стало ясно, что нам нужно остановить выполнение заданий, которые записывают метаданные о таких вещах, как push.  
22 октября 2018 г., 00:05 UTC - Инженеры, задействованные в группе реагирования на инциденты, начали разработку плана устранения несоответствий данных и реализации наших процедур аварийного переключения для MySQL.  
22 октября 2018 г., 00:41 UTC - К этому времени был запущен процесс резервного копирования всех затронутых кластеров MySQL, и инженеры следили за его выполнением.  
22 Октября 2018 г., 06:51 UTC - Несколько кластеров завершили восстановление из резервных копий в нашем центре обработки данных на восточном побережье США и начали репликацию новых данных с западного побережья.  
22 октября 2018 г., 07:46 UTC - GitHub опубликовал сообщение в блоге, чтобы предоставить больше контекста.  
22 октября 2018, 11:12 UTC - Все первичные базы данных снова установлены на Восточном побережье США.  
22 октября 2018 г., 13:15 UTC - Ранее во время инцидента мы начали подготовку дополнительных реплик чтения MySQL в общедоступном облаке Восточного побережья США. Как только они стали доступными, стало проще распределять объем запросов на чтение по большему количеству серверов. Снижение совокупного использования реплик чтения позволило репликации наверстать упущенное.  
22 октября 2018 г., 16:24 UTC - На этом этапе восстановления мы должны были сбалансировать увеличившуюся нагрузку, представленную отставанием, потенциально перегружая наших партнеров по экосистеме уведомлениями и возвращая наши услуги на 100% как можно быстрее. В очереди было более пяти миллионов событий хуков и 80 тысяч сборок страниц.  
Октябрь 2018 г., 22 23:03 UTC - Все незавершенные веб-перехватчики и сборки страниц были обработаны, и была подтверждена целостность и правильная работа всех систем.

9. Последующие действия (что нужно предпринять, чтобы инцидент не повторялся):  
Устранение несоответсвия в данных.Было сделано несколько общедоступных оценок времени ремонта на основе скорости обработки накопившихся данных. Также были приняты ряд технических иницатив.
